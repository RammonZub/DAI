# Computer Price Prediction & Market Analysis System

## 1. Project Overview
This project is an end-to-end Machine Learning system designed to predict computer prices based on hardware specifications with high precision. It integrates a rigorous data science pipelineâ€”from raw data cleaning to advanced model optimizationâ€”into a production-grade Streamlit application. The system features real-time inference, explainable AI (SHAP), market segmentation clustering, and an agentic chatbot powered by Google Gemini.

---

## 2. The Machine Learning Pipeline (`Design_AI_v2.ipynb`)
The core intelligence was developed through a structured 8-stage pipeline within the research notebook.

### Phase I: Data Engineering
*   **Data Aggregation**: Merged three disparate datasets (Laptops, Desktops, Workstations) and enriched them with external CPU/GPU benchmark scores.
*   **Cleaning & Regex**: Implemented complex Regular Expressions to parse unstructured text fields (e.g., extracting "16GB" from "16 GB DDR4 3200MHz").
*   **Imputation Strategy**: Addressed missing values using statistical imputation. Median values were used for numerical features (RAM, Battery) to resist outliers, while mode was used for categorical features.

### Phase II: Feature Engineering
*   **Encoding**: Applied **Target Encoding** for high-cardinality features (CPU/GPU Model Names) to capture their relationship with price, and **One-Hot Encoding** for low-cardinality nominal features (OS, Brand).
*   **Scaling**: Standardized numerical features using `StandardScaler` ($\mu=0, \sigma=1$) to ensure distance-based algorithms (like KNN and SVR) performed correctly.
*   **Feature Selection**: Utilized Random Forest Feature Importance to reduce dimensionality. We selected the top 11 most predictive features (including `cpu_mark`, `gpu_mark`, `ram_gb`, `ssd_gb`, `weight_kg`) to prevent overfitting.

### Phase III: Model Selection & Optimization
*   **Benchmarking**: We rigorously tested 10 different regression algorithms using K-Fold Cross-Validation:
    *   *Linear Models*: Linear Regression, Ridge, Lasso
    *   *Tree-Based*: Decision Trees, Random Forest, **Gradient Boosting**, XGBoost, LightGBM, ExtraTrees
    *   *Other*: SVR, KNN
*   **The Winner**: **Gradient Boosting Regressor** achieved the highest $R^2$ score (0.72) and lowest RMSE.
*   **Hyperparameter Tuning**: We optimized the winning model with specific parameters to balance bias and variance:
    *   `n_estimators`: 200 (Number of boosting stages)
    *   `learning_rate`: 0.15 (Step size shrinkage)
    *   `max_depth`: 5 (Tree complexity)
    *   `min_samples_leaf`: 2 (Regularization)

---

## 3. System Architecture & Orchestration
The application follows a strict separation of concerns between Offline Training and Online Inference.

### The Orchestration Pipeline
1.  **Offline Training (`train_model.py`)**: 
    *   Loads raw data -> Cleans -> Trains the Gradient Boosting model.
    *   **Serialization**: "Freezes" the state of the model and the schema metadata (medians, encodings) into binary files in the `models/` directory.
2.  **Persistence Layer (`models/`)**: 
    *   `price_predictor.joblib`: The serialized fitted model object.
    *   `feature_info.json`: The schema "contract" containing imputation statistics and encoding maps.
    *   `input_options.json`: Valid values for UI dropdowns.
3.  **Online Inference (`app.py` + `app_utils.py`)**: 
    *   **Initialization**: Loads the artifacts from the Persistence Layer into memory (Cached).
    *   **Vectorization**: `preprocess_input()` transforms raw user input into a numerical vector using the JSON schema laws.
    *   **Scoring**: The vector is passed to the loaded model for sub-50ms prediction.

---

## 4. Project Structure & File Manifest

### ðŸ“‚ Root Directory
*   **`train_model.py`**: The "Factory". Runs the full ETL and Training pipeline. Execute this to regenerate model artifacts.
*   **`app.py`**: The "Frontend". The main Streamlit entry point handling layout, navigation, and user inputs.
*   **`app_utils.py`**: The "Engine". Contains all core business logic, including data loading, preprocessing functions, KNN search, K-Means clustering, and Agent tool definitions.

### ðŸ“‚ Directories
*   **`db/`**: Contains the cleaned dataset (`db_computers_clean.csv`) acting as the knowledge base for search and segmentation.
*   **`models/`**: Stores the serialization artifacts (`.joblib`, `.json`) generated by `train_model.py`.
*   **`pages/`**:
    *   **`1_Feature_Importance.py`**: Visualizes model interpretability using global feature importance charts.
    *   **`2_Chatbot.py`**: Implements the Agentic AI interface using Google Gemini 2.5 Flash and Function Calling.

---

## 5. Deployment & Usage

### Prerequisites
*   Python 3.10+
*   Dependencies listed in `requirements.txt`

### Installation
```bash
pip install -r requirements.txt
```

### Running the System
1.  **Regenerate Model (Optional)**: If data changes, retrain the model.
    ```bash
    python train_model.py
    ```
2.  **Launch Application**: Start the web server.
    ```bash
    streamlit run app.py
    ```

---

## 6. Advanced Technical Features
*   **Explainable AI (XAI)**: We implement **SHAP (Shapley Additive Explanations)** in real-time. For every prediction, a `TreeExplainer` calculates the marginal contribution of each feature, generating a Waterfall plot that explains *why* a price was predicted.
*   **Agentic Workflow**: The Chatbot is grounded in determinism. It maps natural language intents to Python function calls (Tools), allowing the LLM to query the database, run predictions, and fetch statistics without hallucination.
